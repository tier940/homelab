# Vector Development Environment Configuration
# Based on AWS FluentBit container insights configuration
role: Agent

# Vector image
# Using debian variant to include systemd tools (journalctl) for journald source
image:
  repository: docker.io/timberio/vector
  tag: 0.51.0-debian
  pullPolicy: IfNotPresent

# Pod labels
podLabels:
  vector.dev/exclude: "true"

# Vector configuration
customConfig:
  data_dir: /var/lib/vector

  # API for health checks
  api:
    enabled: true
    address: 0.0.0.0:8686
    playground: false

  sources:
    # Application logs - Kubernetes container logs
    kubernetes_logs:
      type: kubernetes_logs
      # Exclude vector's own logs and test pods to avoid recursion and noise
      exclude_paths_glob_patterns:
        - "**/vector-*.log"
      extra_label_selector: "vector.dev/exclude!=true"

    # Data plane logs - systemd journal for core services
    dataplane_logs:
      type: journald
      journal_directory: /var/log/journal
      current_boot_only: true
      include_units:
        - sshd.service
        - crio.service
        - kubelet.service

    # Host logs - system logs
    host_dmesg:
      type: file
      include:
        - /var/log/dmesg
      read_from: beginning

    host_secure:
      type: file
      include:
        - /var/log/secure
      read_from: end
      ignore_older_secs: 86400 # 1 day

    host_messages:
      type: file
      include:
        - /var/log/messages
      read_from: end
      ignore_older_secs: 86400 # 1 day

    # Vector internal metrics
    internal_metrics:
      type: internal_metrics

  transforms:
    # Parse and enrich Kubernetes application logs
    parse_application_logs:
      type: remap
      inputs: ["kubernetes_logs"]
      source: |
        # Store pod_name for later use
        pod_name = .kubernetes.pod_name || "unknown"

        # Parse JSON logs and extract only message + level
        if is_string(.message) {
          parsed, err = parse_json(.message)
          if err == null {
            if exists(parsed.message) {
              .message = parsed.message
            }
            if exists(parsed.level) {
              .log_level = downcase(string!(parsed.level))
            } else if exists(parsed.severity) {
              .log_level = downcase(string!(parsed.severity))
            }
          }
        }

        # Extract log level if not found in JSON
        if !exists(.log_level) {
          if exists(.level) {
            .log_level = downcase(string!(.level))
          } else if exists(.severity) {
            .log_level = downcase(string!(.severity))
          } else {
            # Try to extract from klog format (e.g., "W1112 09:44:48" or "E1112 09:44:48")
            msg = string!(.message)
            if starts_with(msg, "E") {
              .log_level = "error"
            } else if starts_with(msg, "W") {
              .log_level = "warning"
            } else if starts_with(msg, "I") {
              .log_level = "info"
            } else {
              .log_level = "info"
            }
          }
        }

        # Normalize warn to warning for consistency
        if .log_level == "warn" {
          .log_level = "warning"
        }

        # Set output fields in desired display order
        # 1. Most important: log level and message (already set above)
        # 2. Kubernetes metadata
        .namespace = .kubernetes.pod_namespace || "unknown"
        .pod = pod_name
        .container = .kubernetes.container_name
        .node = .kubernetes.pod_node_name

        # Remove ALL fields that are already in labels or not needed
        del(.kubernetes)
        del(.file)
        del(.source_type)
        del(.stream)
        del(.timestamp_end)

    # Parse dataplane logs (systemd journal)
    parse_dataplane_logs:
      type: remap
      inputs:
        - "dataplane_logs"
      source: |
        # Extract message field first
        if exists(.MESSAGE) {
          .message = string!(.MESSAGE)
        }

        # Extract log level from message (e.g., level=info, level=error)
        if is_string(.message) {
          msg = string!(.message)
          if match(msg, r'level=(error|ERROR)') {
            .log_level = "error"
          } else if match(msg, r'level=(warn|WARN|warning|WARNING)') {
            .log_level = "warning"
          } else if match(msg, r'level=(info|INFO)') {
            .log_level = "info"
          } else {
            .log_level = "info"
          }
        } else {
          .log_level = "info"
        }

        # Extract hostname for metadata
        host = ""
        if exists(._HOSTNAME) {
          host = string!(._HOSTNAME)
        } else {
          host = get_env_var("VECTOR_SELF_NODE_NAME") ?? "unknown"
        }

        # Container name from systemd unit
        container_name = ""
        if exists(._SYSTEMD_UNIT) {
          container_name = string!(._SYSTEMD_UNIT)
        } else {
          container_name = "journald"
        }

        # Set output fields in desired display order
        # 1. log_level and message (already set above)
        # 2. System metadata
        .namespace = "system"
        .pod = host
        .container = container_name
        .host = host

    # Parse host logs
    parse_host_logs:
      type: remap
      inputs:
        - "host_dmesg"
        - "host_secure"
        - "host_messages"
      source: |
        # Default log level for host logs
        .log_level = "info"

        # Determine container name from file path
        container_name = "unknown"
        if exists(.file) {
          if contains(string!(.file), "dmesg") {
            container_name = "dmesg"
          } else if contains(string!(.file), "secure") {
            container_name = "secure"
          } else if contains(string!(.file), "messages") {
            container_name = "messages"
          }
        }

        # Get hostname for metadata
        host = get_env_var("VECTOR_SELF_NODE_NAME") ?? "unknown"

        # Set output fields in desired display order
        # 1. log_level and message (message already exists, log_level set above)
        # 2. System metadata
        .namespace = "system"
        .pod = host
        .container = container_name
        .host = host

        # Clean up
        del(.file)

  sinks:
    # Loki sink for all logs
    loki:
      type: loki
      inputs:
        - parse_application_logs
        - parse_dataplane_logs
        - parse_host_logs
      endpoint: "http://loki.kube-prometheus-stack.svc.cluster.local:3100"
      encoding:
        codec: json
      labels:
        job: vector
        namespace: '{{ "{{" }} namespace {{ "}}" }}'
        pod: '{{ "{{" }} pod {{ "}}" }}'
        container: '{{ "{{" }} container {{ "}}" }}'
        level: '{{ "{{" }} log_level {{ "}}" }}'
      healthcheck:
        enabled: true

# Additional volume mounts for systemd journal and host logs
extraVolumeMounts:
  - name: journal
    mountPath: /var/log/journal
    readOnly: true
  - name: dmesg
    mountPath: /var/log/dmesg
    readOnly: true

extraVolumes:
  - name: journal
    hostPath:
      path: /var/log/journal
  - name: dmesg
    hostPath:
      path: /var/log/dmesg

# Resources
resources:
  requests:
    cpu: 200m
    memory: 256Mi
  limits:
    cpu: 1000m
    memory: 512Mi

# Service for API
service:
  enabled: true
  type: ClusterIP

# Liveness probe
livenessProbe:
  httpGet:
    path: /health
    port: api
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Readiness probe
readinessProbe:
  httpGet:
    path: /health
    port: api
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

# Tolerations for DaemonSet - run on all nodes
tolerations:
  - operator: Exists

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# Termination grace period
terminationGracePeriodSeconds: 60

# DNS policy
dnsPolicy: ClusterFirst

# Environment variables
env:
  - name: VECTOR_LOG
    value: "info"

# PodMonitor for Prometheus metrics collection
podMonitor:
  enabled: true
  jobLabel: app.kubernetes.io/name
  port: prom-exporter
  path: /metrics
  interval: 30s
  honorLabels: false
  honorTimestamps: true
